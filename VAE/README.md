# Variational AutoEncode (VAE)

## Part 1

### Ressource seen to understand VAE

Variational Autoencoders - EXPLAINED! (Video) :
https://www.youtube.com/watch?v=fcvYpzHmhvA&ab_channel=CodeEmporium

My intuition at first : To generate new sample using the Decoder part of an Auto Encoder we need, to understand the sample distribution of the latent variable, to understand the sample distribution of the latent variables.

- AutoEncoder as themselves cannot be used to generate images. Minimize the reconstruction loss.
- VAE : Is able to understand the distribution of the latent variables. Minimize the reconstruction loss + latent loss ("ensure that all the pools ( the distribution) learn by the network are within the same region). We assume the pool follow a normal distribution. So during testing time latent vectors are sampled from Gaussian Mixture.

L17.5 Un auto-encodeur variationnel pour les chiffres manuscrits dans PyTorch - Exemple de code (Video):
https://www.youtube.com/watch?v=afNuE5z2CQ8&ab_channel=SebastianRaschka
slide : https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L17_vae__slides.pdf

Seeing a version of an autoencoder could give me some better understanding.
Important at the end of the encoder don't directly map the flattened vector to the latent dimension.
Flatten it then two vectors :
ex :
nn.Flatten() # End of the encoder part
self.z_mean = torch.nn.Linear(3136,2)
self.z_log_var = torch.nn.Linear(3136,2)

#Start of the decoder part

Log-Var Trick
z = mu + sigma \* epsilon (epsilon following a Normal distribution)
eps is used because we want a continuous distribution, VAE must ensure that points in the neighborhood encode the same image so that
when decoding they produce the same image.
Learn the mean and the standard deviation as part during the training

Instance of sigma being the variance, we use the log-var vector to allow for positive and negative value : log(sigma^2)
We ca use it because sigma = exp(log^2(sigma)/2)
So the resample is z = mu + sigma \* exp(log^2(sigma)/2)

Loss Function
Want to minimize the ELBO (Evidence lower bound), consisting of KL term and reconstruction loss
-> if you assume p\_{w}(x|z) follows multivariate-Bernouilli, use cross entropy
-> if you assume it follows normal distribution, use MSE (seems better since for example pixel value are float between 0 and 1)
MSE is the same as cross-entropy between the empirical distribution and a Gaussian model
(Deep Learning (by Ian Goodfellow, Yoshua Bengio and Aaron Courville))

Using Cross Entropy for value between 0 and 1, gave us loss even when we got the prediction and the loss depend of the value of the pixel ( see slide and explanation)

Loss = Exp*ected neg. log likelihood term; wrt to encoder distribution (Reconstruction) + KullBack-Leibler divergence term where p(z) is the normal standard distribution
L = alpha*L1 + L2
alpha = An hyperparameter ( how much should the model focus on the reconstruction loss and how much it should focus on the KL divergence term)
Minimize KL divergence : ensures latent space is standard normally distributed and continuous
Encoder : q\_{w}(z x)
Decoder : p\_{w}(x|z)

Latent Space Arithmetic :
Exemple
zs : Smile vector : Average embedding of all people with a smile latent representation
zn : No-Smile vector : Average embedding of all people without a smile latent representation
zdiff = zs - zn
-> Give a sad personn a smile by znew = zorign + alpha \* zdiff

After this video what I think is the most important is the use of KL divergence loss to be sure that the distribution of the latent space is normally
distributed which will make us be able to generate sample using the decoder in the inference phase.
L2 = Dkl[N(mu,sigma) || N(0,1)] = -(1/2) \* sum (1 + log(sigma^2) - mu^2 - sigma^2)

To look after :
Conditional VAE

Lecture 21: Variational Autoencoders
https://www.youtube.com/watch?v=LzEywGCT7-A&ab_channel=CarnegieMellonUniversityDeepLearning

The data are generated by draws from the distribution
-> ie the generating process draws from the distribution

Assumption : The world is a boring place
-> The data you have observed are very typical of the process

Select the distribution that has the highest probability of generating the data
-> Should assign lower probability to less frequent observations and vice versa

(Extern definition)
The goal of the maximum likelihood principle is to fit an optimal statistical distribution to some data. This makes the data easier to work with, makes it more general, allows us to see if new data follows the same distribution as the previous data, and lastly, it allows us to classify unlabelled data points.

Maximum likelihood principle:

- We to find the parameter theta that gave us the best fitting to a distribution
  argmax{theta}(P(X,theta)) = argmax{theta} log(P(X,theta)) because log is increasing

Gaussian Mixture : Sum of weighted gausian model with different mean and sd.

Stanford CS229: Machine Learning | Summer 2019 | Lecture 20 - Variational Autoencoder
https://www.youtube.com/watch?v=-TPFg-RG-KY&ab_channel=StanfordOnline

Avis : Good first introduction, not deep enough on the how it works

Resume :
The difference between the VAE and a normal AE is that we force the latent dimension distribution to be a standard Gaussian distribution.
To do that we want our model to learn to encode image to a latent space that we want to be standard gaussian distribution we use the KL divergence loss to ensure that our parameter mu and sigma learned by our model will be that of a standard gaussian distribution ( mean = 0 and std = 1).

We then generate a sample from our gaussian distribution (using the reparametrization trick) and use this sample to try to reconstitute the original image.
